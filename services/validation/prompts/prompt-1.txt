**Objective:** Transform raw data into a structured JSON format according to the provided schema, intelligently inferring and filling in missing values.

**Target Schema:**

```
platform: (string, options: instagram|twitter|news|reddit|other)
content: {
  title: (string, nullable)
  description: (string, required - main text/caption)
  url: (string, required - source/post/article URL)
  media: (array of strings, required - image/video URLs)
  location: (string, nullable - extracted location)
  author: (string, nullable - username/author)
  published_at: (ISO datetime, required - post/article publish time)
  scraped_at: (ISO datetime, required - when data was scraped)
}
metrics: {
  keywords: (string with comma seperated keywords, required - detected keywords)
  entities: (array of strings, required - civic bodies like BBMP, BESCOM, etc.)
  mentions: (integer, required - count of key entity mentions)
  has_location: (boolean, required - true if location is present)
  has_media: (boolean, required - true if images/videos exist)
  has_author: (boolean, required - true if author is known)
  content_length: (integer, required - length of description/text)
  relevance_score: (float, required - initial heuristic score 0.0-1.0)
  embedding: (any, nullable - placeholder, leave as null)
  text_for_embedding: (string, required - format: "keywords + location + title + entities")
}
status: {
  weighted_score: (float, required - calculated via weighted schema)
  ai_score: (float, required - AI validation score, leave as 0.0)
  verified: (boolean, required - default False)
}
```

**Processing Instructions:**

1.  **Analyze Raw Data:** Thoroughly examine the provided input data.
2.  **Populate Schema:** Map all extractable information to the correct fields in the schema.
3.  **Infer and Complete Data:**
    *   **Platform:** If not explicit, deduce from URL or content (default to "other" if impossible).
    *   **Content:**
        *   **Title:** Extract or infer a concise title. Set to `null` if none exists.
        *   **Description:** Use the main text content.
        *   **URL:** Ensure it's a valid source URL.
        *   **Media:** List all image/video URLs. Use an empty array `[]` if none.
        *   **Location:** Extract any geographical references. Set to `null` if absent.
        *   **Author:** Extract author/username. Set to `null` if absent.
        *   **Published At:** Convert dates/times to ISO 8601 format (e.g., `YYYY-MM-DDTHH:MM:SSZ`). Make a reasonable guess (e.g., midnight) if only the date is available.
        *   **Scraped At:** Record the current date and time in ISO 8601 format.
    *   **Metrics:**
        *   **Keywords:** Identify key topical terms.
        *   **Entities:** Specifically find "BBMP", "BESCOM", and other civic entities.
        *   **Mentions:** Count occurrences of identified `entities` within the `description`.
        *   **has_location:** `true` if `location` was found, `false` otherwise.
        *   **has_media:** `true` if `media` array is not empty, `false` otherwise.
        *   **has_author:** `true` if `author` was found, `false` otherwise.
        *   **content_length:** Character count of the `description`.
        *   **relevance_score:** Assign a score (0.0-1.0) based on perceived importance to civic issues.
        *   **embedding:** Keep as `null`.
        *   **text_for_embedding:** Concatenate: `platform` + `location` (if exists) + `title` (if exists) + `entities` (if exists). Omit missing components.
    *   **Status:**
        *   **weighted_score:** Keep as `0.0` (to be computed later).
        *   **ai_score:** Keep as `0.0`.
        *   **verified:** Default to `False`.

4.  **Data Type Consistency:** Ensure all data adheres strictly to the defined types.

5.  **Output:** Produce the final result as a single, well-formatted JSON object.

---

**Input Raw Data:**

```
{raw}
```

Provide **ONLY** JSON Object. 
Dont provide any extra text. 
START AND END WITH JSON BRACKETS.